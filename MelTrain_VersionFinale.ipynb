{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b108bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import argparse\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93885d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, LayerNormalization\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, GlobalAveragePooling2D, Dense, Softmax, Dropout, Flatten, LSTM, Reshape, Conv1D, Bidirectional, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import kapre\n",
    "from kapre.composed import get_melspectrogram_layer\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8629d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'src_root':'cleanwavs',\n",
    "      'batch_size':16,\n",
    "     'delta_time':1.0,\n",
    "     'model_type':'conv2d',\n",
    "     'sample_rate':16000,\n",
    "      'fn':'3a3d0279'\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd17d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2D(N_LABELS=2, SR=16000, DT=1.0):\n",
    "    input_shape = (int(SR*DT), 1)\n",
    "    i = get_melspectrogram_layer(input_shape=input_shape,\n",
    "                                 n_mels=128,\n",
    "                                 pad_end=True,\n",
    "                                 n_fft=512,\n",
    "                                 win_length=400,\n",
    "                                 hop_length=160,\n",
    "                                 sample_rate=SR,\n",
    "                                 return_decibel=True,\n",
    "                                 input_data_format='channels_last',\n",
    "                                 output_data_format='channels_last')\n",
    "    \n",
    "    # même size\n",
    "    k = tf.keras.layers.experimental.preprocessing.Resizing(150, 150)(i.output)\n",
    "\n",
    "    k = tf.keras.layers.Conv2D(3, (3, 3), padding=\"same\")(k)\n",
    "    # print(i.shape)\n",
    "    pretrained_model = tf.keras.applications.InceptionV3(input_shape=(150, 150, 3),\n",
    "                                                         include_top=False,\n",
    "                                                         weights='imagenet')\n",
    "\n",
    "    pretrained_model.trainable = False \n",
    "    last_layer = pretrained_model.get_layer('mixed7')\n",
    "    x = pretrained_model(k)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.2, name='dropout')(x)\n",
    "    x = layers.Dense(64, activation='relu',\n",
    "                     activity_regularizer=l2(0.001))(x)\n",
    "    o = layers.Dense(N_LABELS, activation='softmax', name='softmax')(x)\n",
    "    model = Model(inputs=i.input, outputs=o, name='2d_convolution')\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b9746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(N_LABELS=2, SR=16000, DT=1.0):\n",
    "    # shape de notre data (n, time, feat)\n",
    "    model_lstm = Sequential()\n",
    "    input_shape = (int(SR*DT), 1)\n",
    "    \n",
    "    i = get_melspectrogram_layer(input_shape=input_shape,\n",
    "                                 n_mels=128,\n",
    "                                 pad_end=True,\n",
    "                                 n_fft=512,\n",
    "                                 win_length=400,\n",
    "                                 hop_length=160,\n",
    "                                 sample_rate=SR,\n",
    "                                 return_decibel=True,\n",
    "                                 input_data_format='channels_last',\n",
    "                                 output_data_format='channels_last',\n",
    "                                 name='2d_convolution')\n",
    "\n",
    "    # model = Sequential()\n",
    "    \n",
    "    model_lstm.add(i)\n",
    "    model_lstm.add(LayerNormalization(axis=2, name='layer_norm'))\n",
    "\n",
    "    model_lstm.add(TimeDistributed(Reshape((-1,)), name='td_reshape'))\n",
    "    model_lstm.add(TimeDistributed(Dense(64, activation='tanh'),\n",
    "                        name='td_ds_tanh'))\n",
    "    model_lstm.add(Bidirectional(layers.GRU(32, return_sequences=True),\n",
    "                             name='bd_lstm'))\n",
    "\n",
    "    model_lstm.add(Bidirectional(layers.GRU(32, return_sequences=True),\n",
    "                             name='bd_lstm_2'))\n",
    "    model_lstm.add(Conv1D(64, 3, activation='relu', name='ds_relu_1'))\n",
    "    model_lstm.add(MaxPooling1D(name='maxp_1d'))\n",
    "    model_lstm.add(Dense(32, activation='relu', name='ds_relu_2'))\n",
    "    model_lstm.add(Flatten(name='flatten'))\n",
    "    model_lstm.add(Dropout(rate=0.2, name='dropout'))\n",
    "    model_lstm.add(Dense(32, activation='relu',\n",
    "                     activity_regularizer=l2(0.001),\n",
    "                     name='ds_relu_3'))\n",
    "\n",
    "\n",
    "    model_lstm.add(Dense(2, activation='softmax'))\n",
    "    model_lstm.summary()\n",
    "    model_lstm.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['acc'])\n",
    "    return model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35498788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f208ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import argparse\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b88695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, wav_paths, labels, sr, dt,n_labels,\n",
    "                 batch_size=32, shuffle=True):\n",
    "        self.wav_paths = wav_paths\n",
    "        self.labels = labels\n",
    "        self.sr = sr\n",
    "        self.dt = dt\n",
    "        self.n_labels = n_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # retourne le nbr de batch dans le dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.wav_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        wav_paths = [self.wav_paths[k] for k in indexes]\n",
    "        labels = [self.labels[k] for k in indexes]\n",
    "\n",
    "        # genere un batch de time data\n",
    "        X = np.empty((self.batch_size, int(self.sr*self.dt), 1),\n",
    "                     dtype=np.float32)\n",
    "        Y = np.empty((self.batch_size, self.n_labels), dtype=np.float32)\n",
    "\n",
    "        for i, (path, label) in enumerate(zip(wav_paths, labels)):\n",
    "            rate, wav = wavfile.read(path)\n",
    "            X[i,] = X[i,][:16000]\n",
    "            # X[i,] = wav.reshape(-1, 1)\n",
    "            for j in range(len(X[i,])):\n",
    "                X[i,][j] = wav.reshape(-1, 1)[j]\n",
    "            Y[i,] = to_categorical(label, num_classes=self.n_labels)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    # mélange les chemins et les étiquettes des fichiers audio à la fin de chaque époque si shufflec'est True.\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.wav_paths))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2074d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    src_root = args['src_root']\n",
    "    sr = args['sample_rate']\n",
    "    dt = args['delta_time']\n",
    "    batch_size = args['batch_size']\n",
    "    model_type = args['model_type']\n",
    "    params = {'N_LABELS': len(os.listdir(src_root)), 'SR': sr, 'DT': dt}\n",
    "    models = {'conv2d': Conv2D(**params), 'lstm': LSTM(**params)}\n",
    "    if model_type not in models.keys():\n",
    "        raise ValueError(f\"{model_type} not an available model\")\n",
    "    csv_path = os.path.join('logs', f'{model_type}_history.csv')\n",
    "\n",
    "    wav_paths = [x.replace(os.sep, '/') for x in glob('{}/**'.format(src_root), recursive=True) if '.wav' in x]\n",
    "    classes = sorted(os.listdir(src_root))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(classes)\n",
    "    labels = [os.path.split(x)[0].split('/')[-1] for x in wav_paths]\n",
    "    labels = le.transform(labels)\n",
    "    wav_train, wav_val, label_train, label_val = train_test_split(wav_paths, labels, test_size=0.1, random_state=10)\n",
    "\n",
    "    if len(label_train) < batch_size:\n",
    "        raise ValueError('Nbr of train audios must be superior than batch_size')\n",
    "    if len(set(label_train)) != params['N_LABELS']:\n",
    "        warnings.warn(f'Found {len(set(label_train))}/{params[\"N_LABELS\"]} classes in training data. Increase the size of data or change random_state.')\n",
    "    if len(set(label_val)) != params['N_LABELS']:\n",
    "        warnings.warn(f'Found {len(set(label_val))}/{params[\"N_LABELS\"]} classes in validation data. Increase the size of data  or change random_state.')\n",
    "\n",
    "    tg = DataGenerator(wav_train, label_train, sr, dt, params['N_LABELS'], batch_size=batch_size)\n",
    "    vg = DataGenerator(wav_val, label_val, sr, dt, params['N_LABELS'], batch_size=batch_size)\n",
    "    model = models[model_type]\n",
    "    cp = ModelCheckpoint(f'models/{model_type}.h5', monitor='val_loss', save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch', verbose=1)\n",
    "    csv_logger = CSVLogger(csv_path, append=False)\n",
    "    model.fit(tg, validation_data=vg, epochs=30, verbose=1, callbacks=[csv_logger, cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fae44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeaf9348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "2d_convolution (Sequential)  (None, 100, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "layer_norm (LayerNormalizati (None, 100, 128, 1)       256       \n",
      "_________________________________________________________________\n",
      "td_reshape (TimeDistributed) (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "td_ds_tanh (TimeDistributed) (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "bd_lstm (Bidirectional)      (None, 100, 64)           18816     \n",
      "_________________________________________________________________\n",
      "bd_lstm_2 (Bidirectional)    (None, 100, 64)           18816     \n",
      "_________________________________________________________________\n",
      "ds_relu_1 (Conv1D)           (None, 98, 64)            12352     \n",
      "_________________________________________________________________\n",
      "maxp_1d (MaxPooling1D)       (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "ds_relu_2 (Dense)            (None, 49, 32)            2080      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "ds_relu_3 (Dense)            (None, 32)                50208     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 110,850\n",
      "Trainable params: 110,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4262 - acc: 0.7826\n",
      "Epoch 00001: val_loss improved from inf to 0.34000, saving model to vers_fin\\lstm.h5\n",
      "69/69 [==============================] - 20s 288ms/step - loss: 0.4262 - acc: 0.7826 - val_loss: 0.3400 - val_acc: 0.8393\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.2447 - acc: 0.9221\n",
      "Epoch 00002: val_loss improved from 0.34000 to 0.10134, saving model to vers_fin\\lstm.h5\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.2447 - acc: 0.9221 - val_loss: 0.1013 - val_acc: 0.9732\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.1405 - acc: 0.9583\n",
      "Epoch 00003: val_loss improved from 0.10134 to 0.07761, saving model to vers_fin\\lstm.h5\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.1405 - acc: 0.9583 - val_loss: 0.0776 - val_acc: 0.9821\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0932 - acc: 0.9764\n",
      "Epoch 00004: val_loss did not improve from 0.07761\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.0932 - acc: 0.9764 - val_loss: 0.0788 - val_acc: 0.9821\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0702 - acc: 0.9900\n",
      "Epoch 00005: val_loss improved from 0.07761 to 0.03462, saving model to vers_fin\\lstm.h5\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.0702 - acc: 0.9900 - val_loss: 0.0346 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.1922 - acc: 0.9212\n",
      "Epoch 00006: val_loss did not improve from 0.03462\n",
      "69/69 [==============================] - 19s 277ms/step - loss: 0.1922 - acc: 0.9212 - val_loss: 0.1111 - val_acc: 0.9464\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0857 - acc: 0.9810\n",
      "Epoch 00007: val_loss did not improve from 0.03462\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.0857 - acc: 0.9810 - val_loss: 0.0358 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0752 - acc: 0.9837\n",
      "Epoch 00008: val_loss did not improve from 0.03462\n",
      "69/69 [==============================] - 19s 277ms/step - loss: 0.0752 - acc: 0.9837 - val_loss: 0.0404 - val_acc: 0.9911\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0688 - acc: 0.9864\n",
      "Epoch 00009: val_loss did not improve from 0.03462\n",
      "69/69 [==============================] - 19s 277ms/step - loss: 0.0688 - acc: 0.9864 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0600 - acc: 0.9918\n",
      "Epoch 00010: val_loss did not improve from 0.03462\n",
      "69/69 [==============================] - 20s 285ms/step - loss: 0.0600 - acc: 0.9918 - val_loss: 0.1592 - val_acc: 0.9464\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0700 - acc: 0.9828\n",
      "Epoch 00011: val_loss improved from 0.03462 to 0.02353, saving model to vers_fin\\lstm.h5\n",
      "69/69 [==============================] - 20s 286ms/step - loss: 0.0700 - acc: 0.9828 - val_loss: 0.0235 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0582 - acc: 0.9891\n",
      "Epoch 00012: val_loss did not improve from 0.02353\n",
      "69/69 [==============================] - 19s 276ms/step - loss: 0.0582 - acc: 0.9891 - val_loss: 0.0590 - val_acc: 0.9911\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0456 - acc: 0.9937\n",
      "Epoch 00013: val_loss did not improve from 0.02353\n",
      "69/69 [==============================] - 19s 280ms/step - loss: 0.0456 - acc: 0.9937 - val_loss: 0.0661 - val_acc: 0.9821\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0457 - acc: 0.9928\n",
      "Epoch 00014: val_loss did not improve from 0.02353\n",
      "69/69 [==============================] - 19s 279ms/step - loss: 0.0457 - acc: 0.9928 - val_loss: 0.0651 - val_acc: 0.9821\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0324 - acc: 0.9973\n",
      "Epoch 00015: val_loss improved from 0.02353 to 0.02284, saving model to vers_fin\\lstm.h5\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.0324 - acc: 0.9973 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0268 - acc: 0.9982\n",
      "Epoch 00016: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 268ms/step - loss: 0.0268 - acc: 0.9982 - val_loss: 0.0352 - val_acc: 0.9911\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.1009 - acc: 0.9755\n",
      "Epoch 00017: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 268ms/step - loss: 0.1009 - acc: 0.9755 - val_loss: 0.0338 - val_acc: 0.9911\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0838 - acc: 0.9783\n",
      "Epoch 00018: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.0838 - acc: 0.9783 - val_loss: 0.1191 - val_acc: 0.9554\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.1010 - acc: 0.9683\n",
      "Epoch 00019: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 269ms/step - loss: 0.1010 - acc: 0.9683 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0383 - acc: 0.9946\n",
      "Epoch 00020: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.0383 - acc: 0.9946 - val_loss: 0.0253 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0908 - acc: 0.9746\n",
      "Epoch 00021: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.0908 - acc: 0.9746 - val_loss: 0.0360 - val_acc: 0.9911\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0574 - acc: 0.9855\n",
      "Epoch 00022: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.0574 - acc: 0.9855 - val_loss: 0.0295 - val_acc: 0.9911\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.9955\n",
      "Epoch 00023: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.0296 - acc: 0.9955 - val_loss: 0.0268 - val_acc: 0.9911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0240 - acc: 0.9982\n",
      "Epoch 00024: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.0240 - acc: 0.9982 - val_loss: 0.0509 - val_acc: 0.9911\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0209 - acc: 0.9982\n",
      "Epoch 00025: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.0209 - acc: 0.9982 - val_loss: 0.0485 - val_acc: 0.9911\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0188 - acc: 0.9991\n",
      "Epoch 00026: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 269ms/step - loss: 0.0188 - acc: 0.9991 - val_loss: 0.0437 - val_acc: 0.9911\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0174 - acc: 0.9991\n",
      "Epoch 00027: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 269ms/step - loss: 0.0174 - acc: 0.9991 - val_loss: 0.0465 - val_acc: 0.9911\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0213 - acc: 0.9982\n",
      "Epoch 00028: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 276ms/step - loss: 0.0213 - acc: 0.9982 - val_loss: 0.0617 - val_acc: 0.9821\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0168 - acc: 0.9982\n",
      "Epoch 00029: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.0168 - acc: 0.9982 - val_loss: 0.0491 - val_acc: 0.9911\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.02284\n",
      "69/69 [==============================] - 19s 269ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9911\n"
     ]
    }
   ],
   "source": [
    "#lstm\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec57ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "2d_convolution (Sequential)  (None, 100, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "layer_norm (LayerNormalizati (None, 100, 128, 1)       256       \n",
      "_________________________________________________________________\n",
      "td_reshape (TimeDistributed) (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "td_ds_tanh (TimeDistributed) (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "bd_lstm (Bidirectional)      (None, 100, 64)           18816     \n",
      "_________________________________________________________________\n",
      "bd_lstm_2 (Bidirectional)    (None, 100, 64)           18816     \n",
      "_________________________________________________________________\n",
      "ds_relu_1 (Conv1D)           (None, 98, 64)            12352     \n",
      "_________________________________________________________________\n",
      "maxp_1d (MaxPooling1D)       (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "ds_relu_2 (Dense)            (None, 49, 32)            2080      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "ds_relu_3 (Dense)            (None, 32)                50208     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 110,850\n",
      "Trainable params: 110,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 8.0384 - accuracy: 0.6504\n",
      "Epoch 00001: val_loss improved from inf to 0.67622, saving model to vers_fin\\conv2d.h5\n",
      "69/69 [==============================] - 53s 769ms/step - loss: 8.0384 - accuracy: 0.6504 - val_loss: 0.6762 - val_accuracy: 0.7321\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7423 - accuracy: 0.6603\n",
      "Epoch 00002: val_loss improved from 0.67622 to 0.66165, saving model to vers_fin\\conv2d.h5\n",
      "69/69 [==============================] - 54s 787ms/step - loss: 0.7423 - accuracy: 0.6603 - val_loss: 0.6617 - val_accuracy: 0.7054\n",
      "Epoch 3/30\n",
      "50/69 [====================>.........] - ETA: 13s - loss: 0.7060 - accuracy: 0.6488"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-381db743ff8e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     44\u001b[0m     model.fit(tg, validation_data=vg,\n\u001b[0;32m     45\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m               callbacks=[csv_logger, cp])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1092\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\hamza\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010c6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafb0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224baf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b803ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66908fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff30a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4df4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e94116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fe807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f1a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734ccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
